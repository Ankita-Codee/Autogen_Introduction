{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39f312e9",
   "metadata": {},
   "source": [
    "# Using Different Models with Autogen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a004f6",
   "metadata": {},
   "source": [
    "## OpenAI - Already seen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57f0145",
   "metadata": {},
   "source": [
    "# Ollama - Local hosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3713dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_core.models import UserMessage\n",
    "from autogen_ext.models.ollama import OllamaChatCompletionClient\n",
    "\n",
    "# Assuming your Ollama server is running locally on port 11434.\n",
    "ollama_model_client = OllamaChatCompletionClient(model=\"llama3.2\")\n",
    "\n",
    "response = await ollama_model_client.create([UserMessage(content=\"Who are you, who created you?\", source=\"user\")])\n",
    "print(response)\n",
    "await ollama_model_client.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5786aee3",
   "metadata": {},
   "source": [
    "finish_reason='stop' content='The capital of France is Paris.' usage=RequestUsage(prompt_tokens=32, completion_tokens=8) cached=False logprobs=None thought=None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27c9a3d",
   "metadata": {},
   "source": [
    "finish_reason='stop' content=\"I am a computer program designed to simulate conversation and answer questions to the best of my ability. I'm a type of artificial intelligence (AI) called a large language model, which means I've been trained on a massive dataset of text from various sources.\\n\\nMy primary function is to assist users with information and tasks, and I do this by processing natural language inputs and generating human-like responses. I don't have personal experiences, emotions, or consciousness like humans do, but I'm designed to be helpful and informative.\\n\\nI was created by a team of researcher-engineers at Meta AI, which is a subsidiary of Meta Platforms, Inc. The development of my technology involved extensive research in natural language processing (NLP), machine learning, and cognitive architectures.\\n\\nMy training data consists of a massive corpus of text from various sources, including books, articles, research papers, and websites. This corpus was used to fine-tune my understanding of language patterns, semantics, and context.\\n\\nI'm constantly learning and improving through interactions with users like you, so I appreciate any feedback or corrections that can help me become a better conversational AI!\" usage=RequestUsage(prompt_tokens=33, completion_tokens=226) cached=False logprobs=None thought=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb0090",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_core.models import UserMessage\n",
    "from autogen_ext.models.ollama import OllamaChatCompletionClient\n",
    "\n",
    "# Assuming your Ollama server is running locally on port 11434.\n",
    "ollama_model_client = OllamaChatCompletionClient(model=\"llama3.2\")\n",
    "\n",
    "response = await ollama_model_client.create([UserMessage(content=\"Who are you, who created you?\", source=\"user\")])\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee8a097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "agent_1 = AssistantAgent(name='MyOllamaAgent',model_client=ollama_model_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232574f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await agent_1.run(task='Tell me something about you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60cccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468544ed",
   "metadata": {},
   "source": [
    "TaskResult(messages=[TextMessage(id='a6b436b1-ca2a-430f-bf42-cb46e2ac0185', source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 7, 5, 5, 19, 6, 598002, tzinfo=datetime.timezone.utc), content='Tell me something about you?', type='TextMessage'), TextMessage(id='ad293544-9fd3-4610-9826-536bf9855c2c', source='MyOllamaAgent', models_usage=RequestUsage(prompt_tokens=55, completion_tokens=136), metadata={}, created_at=datetime.datetime(2025, 7, 5, 5, 19, 16, 631317, tzinfo=datetime.timezone.utc), content=\"I'm an artificial intelligence designed to assist and communicate with users like you. I have been trained on a vast amount of text data, which enables me to understand and respond to a wide range of questions and topics.\\n\\nMy primary function is to provide helpful and accurate information, answer questions, and engage in conversations to the best of my abilities. I can process natural language inputs and generate human-like responses.\\n\\nI don't have personal experiences, emotions, or opinions like humans do, but I'm designed to be neutral and informative. My goal is to help users find the information they need, learn new things, or simply have a conversation.\\n\\nWhat would you like to talk about?\", type='TextMessage')], stop_reason=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fbb9cb",
   "metadata": {},
   "source": [
    "# GEMINI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "615f0851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish_reason='stop' content='Paris\\n' usage=RequestUsage(prompt_tokens=7, completion_tokens=2) cached=False logprobs=None thought=None\n"
     ]
    }
   ],
   "source": [
    "# Import the UserMessage model to structure user inputs\n",
    "from autogen_core.models import UserMessage\n",
    "\n",
    "# Import the OpenAIChatCompletionClient (used for interacting with LLMs like Gemini)\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "# Import tools to load environment variables (for security)\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from a .env file (e.g., GOOGLE_API_KEY)\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the model client with the Gemini model using your Google API key\n",
    "# This creates an async client to interact with the Gemini 1.5 Flash model\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gemini-1.5-flash-8b\",\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Send a single user message to the model and await its response\n",
    "# `create()` is a low-level async call that sends one message and waits for a reply\n",
    "response = await model_client.create([\n",
    "    UserMessage(content=\"What is the capital of France?\", source=\"user\")\n",
    "])\n",
    "\n",
    "# Print the model's response (should output something like \"Paris\")\n",
    "print(response)\n",
    "\n",
    "# Close the model client to properly release network resources\n",
    "# This is important in async workflows to avoid unclosed session warnings or resource leaks\n",
    "await model_client.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e544a",
   "metadata": {},
   "source": [
    "``` await model_client.close()```\n",
    "## ✅ Why This Line Is Important:\n",
    "This closes the underlying HTTP session or socket connections used to communicate with the LLM API.\n",
    "\n",
    "Helps prevent:\n",
    "\n",
    "- Memory leaks\n",
    "\n",
    "- Open handles or sockets lingering\n",
    "\n",
    "- Runtime warnings about unclosed sessions\n",
    "\n",
    "Especially critical in async-based environments where connections are not automatically garbage-collected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75178e85",
   "metadata": {},
   "source": [
    "# Open Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e87265c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish_reason='stop' content='The capital of France is Paris.' usage=RequestUsage(prompt_tokens=15, completion_tokens=8) cached=False logprobs=None thought=None\n"
     ]
    }
   ],
   "source": [
    "# API key for OpenRouter (example key shown — always keep your real key secure!)\n",
    "open_router_api_key = os.getenv(\"OPEN_ROUTER_API_KEY\")\n",
    "#print(open_router_api_key)\n",
    "\n",
    "open_router_model_client =  OpenAIChatCompletionClient(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    model=\"qwen/qwen3-coder:free\",\n",
    "    api_key = open_router_api_key,\n",
    "    model_info={\n",
    "        \"family\":'deepseek',\n",
    "        \"vision\" :True,\n",
    "        \"function_calling\":True,\n",
    "        \"json_output\": False\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "response = await open_router_model_client.create([UserMessage(content=\"What is the capital of France?\", source=\"user\")])\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e491dcd",
   "metadata": {},
   "source": [
    "## ✅ Key Difference Between `model_client.create(...)` vs `model_client.run(...)`\n",
    "\n",
    "| Function                | `model_client.create(...)`                          | `model_client.run(...)`                                 |\n",
    "|------------------------|----------------------------------------------------|----------------------------------------------------------|\n",
    "| 🔧 **Abstraction Level** | Low-level, asynchronous                            | High-level, often synchronous                            |\n",
    "| 📤 **Behavior**         | Sends a **single message**, waits for LLM response | Runs an **entire interaction** or multi-turn workflow     |\n",
    "| 🎯 **Use Case**         | Fine-grained control of message-passing            | Encapsulated agent logic and task execution              |\n",
    "| 📦 **Typical Context**  | LLM client libraries (e.g., OpenAI, Groq SDKs)     | Agent frameworks like **AutoGen**, LangChain             |\n",
    "| 🔄 **Flexibility**      | Developer manages context, memory, retries         | Often handles context internally                         |\n",
    "| 🛠️ **Example Scenario** | Custom prompt injection, streaming, etc.           | Full conversation/session handled in one call            |\n",
    "\n",
    "> ⚙️ Use `create(...)` when you need low-level control (e.g., streaming, retries, chaining).  \n",
    "> 🤖 Use `run(...)` when building structured agent flows or invoking autonomous task execution.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
